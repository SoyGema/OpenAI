{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ATARI Asteroids DQN_gym with keras-rl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as no\n",
    "import gym\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.agents.ddpg import DDPGAgent\n",
    "from rl.policy import BoltzmannQPolicy , LinearAnnealedPolicy , EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME_2 = 'Asteroids-v0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the environment and extract the number of actions\n",
    "env = gym.make(ENV_NAME_2)\n",
    "nb_actions = env.action_space.n\n",
    "nb_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 100800)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 302403    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 14)                56        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 14)                0         \n",
      "=================================================================\n",
      "Total params: 302,459\n",
      "Trainable params: 302,459\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Next, we build a neural network model\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "model.add(Dense(3, activation= 'tanh')) # One layer of 3 units with tanh activation function \n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('sigmoid')) # one layer of 1 unit with sigmoid activation function\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DQN -- Deep Reinforcement Learning \n",
    "\n",
    "#Configure and compile the agent. \n",
    "#Use every built-in Keras optimizer and metrics!\n",
    "memory = SequentialMemory(limit=20000, window_length=1)\n",
    "policy = BoltzmannQPolicy()\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=10,\n",
    "              target_model_update=1e-2, policy=policy)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae', 'acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 20000 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/rl/memory.py:29: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   775/20000: episode: 1, duration: 25.397s, episode steps: 775, steps per second: 31, episode reward: 480.000, mean reward: 0.619 [0.000, 100.000], mean action: 6.365 [0.000, 13.000], mean observation: 2.692 [0.000, 240.000], loss: 10.828399, mean_absolute_error: 0.647379, mean_q: 0.806558\n",
      "  2576/20000: episode: 2, duration: 54.979s, episode steps: 1801, steps per second: 33, episode reward: 1510.000, mean reward: 0.838 [0.000, 100.000], mean action: 6.421 [0.000, 13.000], mean observation: 2.110 [0.000, 240.000], loss: 23.661833, mean_absolute_error: 0.869386, mean_q: 0.936581\n",
      "  3395/20000: episode: 3, duration: 24.540s, episode steps: 819, steps per second: 33, episode reward: 830.000, mean reward: 1.013 [0.000, 100.000], mean action: 6.560 [0.000, 13.000], mean observation: 2.360 [0.000, 240.000], loss: 27.443249, mean_absolute_error: 0.932018, mean_q: 0.973010\n",
      "  4308/20000: episode: 4, duration: 27.419s, episode steps: 913, steps per second: 33, episode reward: 1180.000, mean reward: 1.292 [0.000, 100.000], mean action: 6.527 [0.000, 13.000], mean observation: 1.824 [0.000, 240.000], loss: 36.483490, mean_absolute_error: 0.962863, mean_q: 0.983046\n",
      "  6350/20000: episode: 5, duration: 63.414s, episode steps: 2042, steps per second: 32, episode reward: 1630.000, mean reward: 0.798 [0.000, 100.000], mean action: 6.336 [0.000, 13.000], mean observation: 1.857 [0.000, 240.000], loss: 33.446964, mean_absolute_error: 0.972915, mean_q: 0.991543\n",
      "  7959/20000: episode: 6, duration: 50.160s, episode steps: 1609, steps per second: 32, episode reward: 1300.000, mean reward: 0.808 [0.000, 100.000], mean action: 6.430 [0.000, 13.000], mean observation: 1.708 [0.000, 240.000], loss: 31.193878, mean_absolute_error: 0.978691, mean_q: 0.995602\n",
      "  8630/20000: episode: 7, duration: 20.409s, episode steps: 671, steps per second: 33, episode reward: 330.000, mean reward: 0.492 [0.000, 50.000], mean action: 6.805 [0.000, 13.000], mean observation: 2.473 [0.000, 240.000], loss: 28.742104, mean_absolute_error: 0.978450, mean_q: 0.997120\n",
      " 11291/20000: episode: 8, duration: 81.474s, episode steps: 2661, steps per second: 33, episode reward: 1280.000, mean reward: 0.481 [0.000, 100.000], mean action: 6.664 [0.000, 13.000], mean observation: 1.479 [0.000, 240.000], loss: 29.622116, mean_absolute_error: 0.982106, mean_q: 0.998392\n",
      " 12914/20000: episode: 9, duration: 50.129s, episode steps: 1623, steps per second: 32, episode reward: 1370.000, mean reward: 0.844 [0.000, 150.000], mean action: 6.606 [0.000, 13.000], mean observation: 1.946 [0.000, 240.000], loss: 28.346079, mean_absolute_error: 0.981757, mean_q: 0.999218\n",
      " 13445/20000: episode: 10, duration: 16.172s, episode steps: 531, steps per second: 33, episode reward: 430.000, mean reward: 0.810 [0.000, 100.000], mean action: 6.546 [0.000, 13.000], mean observation: 2.587 [0.000, 240.000], loss: 27.548046, mean_absolute_error: 0.980010, mean_q: 0.999444\n",
      " 15722/20000: episode: 11, duration: 70.131s, episode steps: 2277, steps per second: 32, episode reward: 1460.000, mean reward: 0.641 [0.000, 100.000], mean action: 6.480 [0.000, 13.000], mean observation: 1.735 [0.000, 240.000], loss: 30.511580, mean_absolute_error: 0.984567, mean_q: 0.999603\n",
      " 17591/20000: episode: 12, duration: 57.354s, episode steps: 1869, steps per second: 33, episode reward: 2100.000, mean reward: 1.124 [0.000, 100.000], mean action: 6.465 [0.000, 13.000], mean observation: 2.511 [0.000, 240.000], loss: 28.908169, mean_absolute_error: 0.982937, mean_q: 0.999726\n",
      " 19898/20000: episode: 13, duration: 69.790s, episode steps: 2307, steps per second: 33, episode reward: 1300.000, mean reward: 0.564 [0.000, 100.000], mean action: 6.515 [0.000, 13.000], mean observation: 1.437 [0.000, 240.000], loss: 28.853455, mean_absolute_error: 0.983699, mean_q: 0.999853\n",
      "done, took 622.534 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x121649748>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Visualize the training during 20000 steps \n",
    "dqn.fit(env, nb_steps=20000, visualize=True, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the model \n",
    "dqn.save_weights('dqn_{}_weights.h5f'.format(ENV_NAME_2), overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: 80.000, steps: 1934\n",
      "Episode 2: reward: 80.000, steps: 1935\n",
      "Episode 3: reward: 80.000, steps: 1939\n",
      "Episode 4: reward: 80.000, steps: 1937\n",
      "Episode 5: reward: 80.000, steps: 1904\n",
      "Episode 6: reward: 80.000, steps: 1912\n",
      "Episode 7: reward: 80.000, steps: 1939\n",
      "Episode 8: reward: 80.000, steps: 1924\n",
      "Episode 9: reward: 80.000, steps: 1947\n",
      "Episode 10: reward: 80.000, steps: 1928\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x141763438>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the algorithm for 10 episodes \n",
    "dqn.test(env, nb_episodes=10, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Another Policy with dqn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr=\"eps\", value_max=.8, value_min=.01,\n",
    "                              value_test=.0,\n",
    "                              nb_steps=100000)\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions,  nb_steps_warmup=10, \n",
    "               policy=policy, test_policy=policy, memory = memory,\n",
    "               target_model_update=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.compile(Adam(lr=1e-3), metrics=['mae', 'acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 20000 steps ...\n",
      "   678/20000: episode: 1, duration: 20.570s, episode steps: 678, steps per second: 33, episode reward: 630.000, mean reward: 0.929 [0.000, 100.000], mean action: 7.835 [0.000, 13.000], mean observation: 2.334 [0.000, 240.000], loss: 33.277592, mean_absolute_error: 0.991395, acc: 0.104385, mean_q: 0.999981, mean_eps: 0.797282\n",
      "  1696/20000: episode: 2, duration: 30.019s, episode steps: 1018, steps per second: 34, episode reward: 980.000, mean reward: 0.963 [0.000, 100.000], mean action: 7.811 [0.000, 13.000], mean observation: 2.143 [0.000, 240.000], loss: 29.905849, mean_absolute_error: 0.985209, acc: 0.111186, mean_q: 0.999962, mean_eps: 0.790627\n",
      "  2221/20000: episode: 3, duration: 14.991s, episode steps: 525, steps per second: 35, episode reward: 780.000, mean reward: 1.486 [0.000, 100.000], mean action: 7.947 [0.000, 13.000], mean observation: 1.946 [0.000, 240.000], loss: 31.834536, mean_absolute_error: 0.989462, acc: 0.118512, mean_q: 0.999972, mean_eps: 0.784532\n",
      "  3483/20000: episode: 4, duration: 37.069s, episode steps: 1262, steps per second: 34, episode reward: 1300.000, mean reward: 1.030 [0.000, 100.000], mean action: 7.914 [0.000, 13.000], mean observation: 1.815 [0.000, 240.000], loss: 29.901495, mean_absolute_error: 0.986856, acc: 0.128095, mean_q: 0.999975, mean_eps: 0.777473\n",
      "  4372/20000: episode: 5, duration: 25.753s, episode steps: 889, steps per second: 35, episode reward: 580.000, mean reward: 0.652 [0.000, 100.000], mean action: 8.054 [0.000, 13.000], mean observation: 2.322 [0.000, 240.000], loss: 30.451595, mean_absolute_error: 0.985907, acc: 0.140994, mean_q: 0.999973, mean_eps: 0.768977\n",
      "  4950/20000: episode: 6, duration: 17.107s, episode steps: 578, steps per second: 34, episode reward: 380.000, mean reward: 0.657 [0.000, 100.000], mean action: 8.014 [0.000, 13.000], mean observation: 2.566 [0.000, 240.000], loss: 32.498781, mean_absolute_error: 0.991904, acc: 0.149113, mean_q: 0.999969, mean_eps: 0.763182\n",
      "  6360/20000: episode: 7, duration: 42.137s, episode steps: 1410, steps per second: 33, episode reward: 1300.000, mean reward: 0.922 [0.000, 100.000], mean action: 7.954 [0.000, 13.000], mean observation: 1.808 [0.000, 240.000], loss: 29.987015, mean_absolute_error: 0.986660, acc: 0.157757, mean_q: 0.999973, mean_eps: 0.755329\n",
      "  7158/20000: episode: 8, duration: 23.565s, episode steps: 798, steps per second: 34, episode reward: 730.000, mean reward: 0.915 [0.000, 100.000], mean action: 8.179 [0.000, 13.000], mean observation: 1.906 [0.000, 240.000], loss: 29.856297, mean_absolute_error: 0.986424, acc: 0.172345, mean_q: 0.999976, mean_eps: 0.746608\n",
      "  7544/20000: episode: 9, duration: 11.233s, episode steps: 386, steps per second: 34, episode reward: 180.000, mean reward: 0.466 [0.000, 50.000], mean action: 8.347 [0.000, 13.000], mean observation: 2.197 [0.000, 240.000], loss: 35.884028, mean_absolute_error: 0.996317, acc: 0.181590, mean_q: 0.999979, mean_eps: 0.741931\n",
      "  9453/20000: episode: 10, duration: 55.969s, episode steps: 1909, steps per second: 34, episode reward: 1610.000, mean reward: 0.843 [0.000, 100.000], mean action: 8.270 [0.000, 13.000], mean observation: 2.134 [0.000, 240.000], loss: 29.406685, mean_absolute_error: 0.986191, acc: 0.193851, mean_q: 0.999987, mean_eps: 0.732866\n",
      "  9930/20000: episode: 11, duration: 14.038s, episode steps: 477, steps per second: 34, episode reward: 360.000, mean reward: 0.755 [0.000, 100.000], mean action: 8.203 [0.000, 13.000], mean observation: 2.945 [0.000, 240.000], loss: 33.632854, mean_absolute_error: 0.991483, acc: 0.213181, mean_q: 0.999993, mean_eps: 0.723441\n",
      " 10738/20000: episode: 12, duration: 23.618s, episode steps: 808, steps per second: 34, episode reward: 980.000, mean reward: 1.213 [0.000, 100.000], mean action: 8.094 [0.000, 13.000], mean observation: 1.663 [0.000, 240.000], loss: 29.275678, mean_absolute_error: 0.987474, acc: 0.219910, mean_q: 0.999987, mean_eps: 0.718365\n",
      " 11833/20000: episode: 13, duration: 32.779s, episode steps: 1095, steps per second: 33, episode reward: 1080.000, mean reward: 0.986 [0.000, 100.000], mean action: 8.100 [0.000, 13.000], mean observation: 1.664 [0.000, 240.000], loss: 31.754317, mean_absolute_error: 0.990313, acc: 0.226826, mean_q: 0.999986, mean_eps: 0.710849\n",
      " 14099/20000: episode: 14, duration: 65.857s, episode steps: 2266, steps per second: 34, episode reward: 1280.000, mean reward: 0.565 [0.000, 100.000], mean action: 8.650 [0.000, 13.000], mean observation: 1.453 [0.000, 240.000], loss: 30.479552, mean_absolute_error: 0.989559, acc: 0.260715, mean_q: 0.999991, mean_eps: 0.697573\n",
      " 15134/20000: episode: 15, duration: 31.214s, episode steps: 1035, steps per second: 33, episode reward: 780.000, mean reward: 0.754 [0.000, 100.000], mean action: 8.304 [0.000, 13.000], mean observation: 2.179 [0.000, 240.000], loss: 31.050372, mean_absolute_error: 0.988732, acc: 0.284783, mean_q: 0.999993, mean_eps: 0.684534\n",
      " 16351/20000: episode: 16, duration: 34.785s, episode steps: 1217, steps per second: 35, episode reward: 1180.000, mean reward: 0.970 [0.000, 200.000], mean action: 8.618 [0.000, 13.000], mean observation: 1.873 [0.000, 240.000], loss: 32.528273, mean_absolute_error: 0.990981, acc: 0.300431, mean_q: 0.999994, mean_eps: 0.675638\n",
      " 17122/20000: episode: 17, duration: 22.794s, episode steps: 771, steps per second: 34, episode reward: 980.000, mean reward: 1.271 [0.000, 100.000], mean action: 8.855 [0.000, 13.000], mean observation: 1.816 [0.000, 240.000], loss: 34.558610, mean_absolute_error: 0.996491, acc: 0.318823, mean_q: 0.999998, mean_eps: 0.667786\n",
      " 18631/20000: episode: 18, duration: 43.596s, episode steps: 1509, steps per second: 35, episode reward: 1440.000, mean reward: 0.954 [0.000, 100.000], mean action: 8.622 [0.000, 13.000], mean observation: 1.876 [0.000, 240.000], loss: 34.057890, mean_absolute_error: 0.994270, acc: 0.309725, mean_q: 1.000000, mean_eps: 0.658780\n",
      "done, took 593.366 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13400deb8>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.fit(env, nb_steps=20000, visualize=True, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: 110.000, steps: 680\n",
      "Episode 2: reward: 110.000, steps: 692\n",
      "Episode 3: reward: 110.000, steps: 685\n",
      "Episode 4: reward: 110.000, steps: 682\n",
      "Episode 5: reward: 110.000, steps: 675\n",
      "Episode 6: reward: 110.000, steps: 676\n",
      "Episode 7: reward: 110.000, steps: 683\n",
      "Episode 8: reward: 110.000, steps: 682\n",
      "Episode 9: reward: 110.000, steps: 680\n",
      "Episode 10: reward: 110.000, steps: 679\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1549c9fd0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.test(env, nb_episodes=10, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 20000 steps ...\n",
      "   533/20000: episode: 1, duration: 4.751s, episode steps: 533, steps per second: 112, episode reward: 340.000, mean reward: 0.638 [0.000, 100.000], mean action: 7.762 [0.000, 12.000], mean observation: 2.698 [0.000, 240.000], loss: 24.707138, mean_absolute_error: 0.975929, acc: 0.898467, mean_q: 1.000000\n",
      "  1046/20000: episode: 2, duration: 3.683s, episode steps: 513, steps per second: 139, episode reward: 160.000, mean reward: 0.312 [0.000, 50.000], mean action: 11.986 [0.000, 13.000], mean observation: 2.752 [0.000, 240.000], loss: 6.052589, mean_absolute_error: 0.951730, acc: 0.898438, mean_q: 1.000000\n",
      "  2008/20000: episode: 3, duration: 6.924s, episode steps: 962, steps per second: 139, episode reward: 830.000, mean reward: 0.863 [0.000, 100.000], mean action: 12.427 [0.000, 13.000], mean observation: 2.225 [0.000, 240.000], loss: 30.741153, mean_absolute_error: 0.991028, acc: 0.920916, mean_q: 1.000000\n",
      "  2641/20000: episode: 4, duration: 4.364s, episode steps: 633, steps per second: 145, episode reward: 430.000, mean reward: 0.679 [0.000, 100.000], mean action: 12.343 [0.000, 13.000], mean observation: 2.713 [0.000, 240.000], loss: 23.016189, mean_absolute_error: 0.977975, acc: 0.912975, mean_q: 1.000000\n",
      "  3095/20000: episode: 5, duration: 3.128s, episode steps: 454, steps per second: 145, episode reward: 210.000, mean reward: 0.463 [0.000, 100.000], mean action: 12.344 [0.000, 13.000], mean observation: 2.479 [0.000, 240.000], loss: 15.117931, mean_absolute_error: 0.962537, acc: 0.913907, mean_q: 1.000000\n",
      "  3535/20000: episode: 6, duration: 3.042s, episode steps: 440, steps per second: 145, episode reward: 210.000, mean reward: 0.477 [0.000, 100.000], mean action: 12.339 [0.000, 13.000], mean observation: 2.513 [0.000, 240.000], loss: 15.600050, mean_absolute_error: 0.963598, acc: 0.899772, mean_q: 1.000000\n",
      "  4032/20000: episode: 7, duration: 3.429s, episode steps: 497, steps per second: 145, episode reward: 280.000, mean reward: 0.563 [0.000, 50.000], mean action: 11.972 [0.000, 13.000], mean observation: 2.724 [0.000, 240.000], loss: 11.688962, mean_absolute_error: 0.969727, acc: 0.909274, mean_q: 1.000000\n",
      "  4451/20000: episode: 8, duration: 2.893s, episode steps: 419, steps per second: 145, episode reward: 160.000, mean reward: 0.382 [0.000, 50.000], mean action: 7.831 [1.000, 13.000], mean observation: 2.873 [0.000, 240.000], loss: 7.413687, mean_absolute_error: 0.956778, acc: 0.911483, mean_q: 1.000000\n",
      "  5058/20000: episode: 9, duration: 4.204s, episode steps: 607, steps per second: 144, episode reward: 110.000, mean reward: 0.181 [0.000, 50.000], mean action: 7.819 [0.000, 13.000], mean observation: 2.907 [0.000, 240.000], loss: 3.051865, mean_absolute_error: 0.942358, acc: 0.912541, mean_q: 1.000000\n",
      "  5641/20000: episode: 10, duration: 4.021s, episode steps: 583, steps per second: 145, episode reward: 280.000, mean reward: 0.480 [0.000, 100.000], mean action: 7.906 [0.000, 13.000], mean observation: 2.541 [0.000, 240.000], loss: 15.975479, mean_absolute_error: 0.963756, acc: 0.924399, mean_q: 1.000000\n",
      "  6122/20000: episode: 11, duration: 3.318s, episode steps: 481, steps per second: 145, episode reward: 410.000, mean reward: 0.852 [0.000, 100.000], mean action: 7.863 [0.000, 13.000], mean observation: 2.697 [0.000, 240.000], loss: 29.888383, mean_absolute_error: 0.990420, acc: 0.889583, mean_q: 1.000000\n",
      "  6542/20000: episode: 12, duration: 2.897s, episode steps: 420, steps per second: 145, episode reward: 190.000, mean reward: 0.452 [0.000, 100.000], mean action: 7.848 [0.000, 13.000], mean observation: 3.217 [0.000, 240.000], loss: 15.867830, mean_absolute_error: 0.961830, acc: 0.911695, mean_q: 1.000000\n",
      "  7142/20000: episode: 13, duration: 4.141s, episode steps: 600, steps per second: 145, episode reward: 560.000, mean reward: 0.933 [0.000, 100.000], mean action: 7.765 [0.000, 13.000], mean observation: 2.662 [0.000, 240.000], loss: 34.382187, mean_absolute_error: 0.996157, acc: 0.896494, mean_q: 1.000000\n",
      "  7590/20000: episode: 14, duration: 3.097s, episode steps: 448, steps per second: 145, episode reward: 480.000, mean reward: 1.071 [0.000, 150.000], mean action: 7.839 [0.000, 13.000], mean observation: 2.318 [0.000, 240.000], loss: 40.930030, mean_absolute_error: 1.006113, acc: 0.917226, mean_q: 1.000000\n",
      "  8393/20000: episode: 15, duration: 5.577s, episode steps: 803, steps per second: 144, episode reward: 440.000, mean reward: 0.548 [0.000, 100.000], mean action: 7.923 [0.000, 13.000], mean observation: 3.037 [0.000, 240.000], loss: 19.197182, mean_absolute_error: 0.968547, acc: 0.911471, mean_q: 1.000000\n",
      "  8877/20000: episode: 16, duration: 3.338s, episode steps: 484, steps per second: 145, episode reward: 290.000, mean reward: 0.599 [0.000, 100.000], mean action: 7.831 [0.000, 13.000], mean observation: 3.213 [0.000, 240.000], loss: 18.939181, mean_absolute_error: 0.972300, acc: 0.910973, mean_q: 1.000000\n",
      "  9541/20000: episode: 17, duration: 4.610s, episode steps: 664, steps per second: 144, episode reward: 160.000, mean reward: 0.241 [0.000, 50.000], mean action: 7.776 [0.000, 13.000], mean observation: 2.968 [0.000, 240.000], loss: 4.674107, mean_absolute_error: 0.946618, acc: 0.906486, mean_q: 1.000000\n",
      " 10248/20000: episode: 18, duration: 4.893s, episode steps: 707, steps per second: 145, episode reward: 140.000, mean reward: 0.198 [0.000, 50.000], mean action: 12.225 [0.000, 13.000], mean observation: 2.689 [0.000, 240.000], loss: 4.106424, mean_absolute_error: 0.943541, acc: 0.915014, mean_q: 1.000000\n",
      " 10872/20000: episode: 19, duration: 4.288s, episode steps: 624, steps per second: 146, episode reward: 430.000, mean reward: 0.689 [0.000, 100.000], mean action: 12.186 [0.000, 13.000], mean observation: 2.576 [0.000, 240.000], loss: 23.348686, mean_absolute_error: 0.978679, acc: 0.894061, mean_q: 1.000000\n",
      " 11848/20000: episode: 20, duration: 7.359s, episode steps: 976, steps per second: 133, episode reward: 730.000, mean reward: 0.748 [0.000, 100.000], mean action: 12.433 [0.000, 13.000], mean observation: 2.121 [0.000, 240.000], loss: 27.736665, mean_absolute_error: 0.982818, acc: 0.916923, mean_q: 1.000000\n",
      " 12269/20000: episode: 21, duration: 2.902s, episode steps: 421, steps per second: 145, episode reward: 430.000, mean reward: 1.021 [0.000, 100.000], mean action: 12.295 [0.000, 13.000], mean observation: 2.104 [0.000, 240.000], loss: 28.681479, mean_absolute_error: 1.002548, acc: 0.892857, mean_q: 1.000000\n",
      " 12794/20000: episode: 22, duration: 3.616s, episode steps: 525, steps per second: 145, episode reward: 430.000, mean reward: 0.819 [0.000, 100.000], mean action: 12.284 [0.000, 13.000], mean observation: 2.504 [0.000, 240.000], loss: 27.759974, mean_absolute_error: 0.988010, acc: 0.906489, mean_q: 1.000000\n",
      " 13227/20000: episode: 23, duration: 3.135s, episode steps: 433, steps per second: 138, episode reward: 130.000, mean reward: 0.300 [0.000, 50.000], mean action: 12.513 [0.000, 13.000], mean observation: 2.539 [0.000, 240.000], loss: 4.743569, mean_absolute_error: 0.950926, acc: 0.918981, mean_q: 1.000000\n",
      " 14091/20000: episode: 24, duration: 5.937s, episode steps: 864, steps per second: 146, episode reward: 630.000, mean reward: 0.729 [0.000, 100.000], mean action: 12.440 [0.000, 13.000], mean observation: 2.340 [0.000, 240.000], loss: 25.543735, mean_absolute_error: 0.981491, acc: 0.922364, mean_q: 1.000000\n",
      " 14870/20000: episode: 25, duration: 5.380s, episode steps: 779, steps per second: 145, episode reward: 430.000, mean reward: 0.552 [0.000, 100.000], mean action: 9.973 [0.000, 13.000], mean observation: 2.498 [0.000, 240.000], loss: 15.483598, mean_absolute_error: 0.968836, acc: 0.897172, mean_q: 1.000000\n",
      " 15410/20000: episode: 26, duration: 3.722s, episode steps: 540, steps per second: 145, episode reward: 410.000, mean reward: 0.759 [0.000, 100.000], mean action: 7.952 [0.000, 13.000], mean observation: 2.629 [0.000, 240.000], loss: 27.358863, mean_absolute_error: 0.983731, acc: 0.914657, mean_q: 1.000000\n",
      " 15906/20000: episode: 27, duration: 3.421s, episode steps: 496, steps per second: 145, episode reward: 210.000, mean reward: 0.423 [0.000, 50.000], mean action: 7.833 [0.000, 13.000], mean observation: 2.596 [0.000, 240.000], loss: 8.784697, mean_absolute_error: 0.959714, acc: 0.907071, mean_q: 1.000000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 16374/20000: episode: 28, duration: 3.227s, episode steps: 468, steps per second: 145, episode reward: 260.000, mean reward: 0.556 [0.000, 150.000], mean action: 7.748 [0.000, 13.000], mean observation: 2.917 [0.000, 240.000], loss: 28.046947, mean_absolute_error: 0.969189, acc: 0.888651, mean_q: 1.000000\n",
      " 16775/20000: episode: 29, duration: 2.769s, episode steps: 401, steps per second: 145, episode reward: 240.000, mean reward: 0.599 [0.000, 100.000], mean action: 7.878 [0.000, 13.000], mean observation: 3.306 [0.000, 240.000], loss: 19.745300, mean_absolute_error: 0.972301, acc: 0.900000, mean_q: 1.000000\n",
      " 17502/20000: episode: 30, duration: 5.010s, episode steps: 727, steps per second: 145, episode reward: 560.000, mean reward: 0.770 [0.000, 100.000], mean action: 7.894 [0.000, 13.000], mean observation: 2.321 [0.000, 240.000], loss: 28.367681, mean_absolute_error: 0.984459, acc: 0.911846, mean_q: 1.000000\n",
      " 18057/20000: episode: 31, duration: 3.828s, episode steps: 555, steps per second: 145, episode reward: 310.000, mean reward: 0.559 [0.000, 100.000], mean action: 7.928 [1.000, 13.000], mean observation: 2.712 [0.000, 240.000], loss: 16.872613, mean_absolute_error: 0.969364, acc: 0.909747, mean_q: 1.000000\n",
      " 18534/20000: episode: 32, duration: 3.305s, episode steps: 477, steps per second: 144, episode reward: 240.000, mean reward: 0.503 [0.000, 100.000], mean action: 7.891 [0.000, 13.000], mean observation: 2.801 [0.000, 240.000], loss: 16.592697, mean_absolute_error: 0.965433, acc: 0.913866, mean_q: 1.000000\n",
      " 19062/20000: episode: 33, duration: 3.647s, episode steps: 528, steps per second: 145, episode reward: 310.000, mean reward: 0.587 [0.000, 100.000], mean action: 7.790 [0.000, 13.000], mean observation: 2.822 [0.000, 240.000], loss: 17.737052, mean_absolute_error: 0.971417, acc: 0.893738, mean_q: 1.000000\n",
      " 19521/20000: episode: 34, duration: 3.207s, episode steps: 459, steps per second: 143, episode reward: 310.000, mean reward: 0.675 [0.000, 100.000], mean action: 7.880 [0.000, 13.000], mean observation: 2.558 [0.000, 240.000], loss: 20.409220, mean_absolute_error: 0.977765, acc: 0.912664, mean_q: 1.000000\n",
      "done, took 146.059 seconds\n",
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: 110.000, steps: 686\n",
      "Episode 2: reward: 110.000, steps: 678\n",
      "Episode 3: reward: 110.000, steps: 690\n",
      "Episode 4: reward: 110.000, steps: 691\n",
      "Episode 5: reward: 110.000, steps: 682\n",
      "Episode 6: reward: 110.000, steps: 671\n",
      "Episode 7: reward: 110.000, steps: 681\n",
      "Episode 8: reward: 110.000, steps: 698\n",
      "Episode 9: reward: 110.000, steps: 679\n",
      "Episode 10: reward: 110.000, steps: 693\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x154b79b70>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SARSA Agent -- Deep Reinforcement Learning \n",
    "from rl.agents.sarsa import SARSAAgent\n",
    "sarsa = SARSAAgent(model, nb_actions, \n",
    "                policy=None, test_policy=None, \n",
    "                gamma=0.99, nb_steps_warmup=10, \n",
    "                train_interval=1)\n",
    "sarsa.compile(Adam(lr=1e-3), metrics=['mae', 'acc'])\n",
    "sarsa.fit(env, nb_steps=20000, visualize=True, verbose=2)\n",
    "sarsa.test(env, nb_episodes=10, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
