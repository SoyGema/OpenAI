{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ATARI Asteroids DQN_gym with keras-rl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as no\n",
    "import gym\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_NAME_2 = 'Asteroids-v0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the environment and extract the number of actions\n",
    "env = gym.make(ENV_NAME_2)\n",
    "nb_actions = env.action_space.n\n",
    "nb_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_4 (Flatten)          (None, 100800)            0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 3)                 302403    \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 14)                56        \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 14)                0         \n",
      "=================================================================\n",
      "Total params: 302,459\n",
      "Trainable params: 302,459\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Next, we build a neural network model\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "model.add(Dense(3, activation= 'tanh')) # One layer of 3 units with tanh activation function \n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('sigmoid')) # one layer of 1 unit with sigmoid activation function\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure and compile the agent. Use every built-in Keras optimizer and metrics!\n",
    "memory = SequentialMemory(limit=20000, window_length=1)\n",
    "policy = BoltzmannQPolicy()\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=10,\n",
    "              target_model_update=1e-2, policy=policy)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 20000 steps ...\n",
      "  1454/20000: episode: 1, duration: 41.667s, episode steps: 1454, steps per second: 35, episode reward: 1030.000, mean reward: 0.708 [0.000, 100.000], mean action: 6.395 [0.000, 13.000], mean observation: 1.887 [0.000, 240.000], loss: 24.489462, mean_absolute_error: 0.974011, mean_q: 0.998769\n",
      "  3841/20000: episode: 2, duration: 68.027s, episode steps: 2387, steps per second: 35, episode reward: 1530.000, mean reward: 0.641 [0.000, 100.000], mean action: 6.554 [0.000, 13.000], mean observation: 1.817 [0.000, 240.000], loss: 24.838045, mean_absolute_error: 0.975894, mean_q: 0.999373\n",
      "  4230/20000: episode: 3, duration: 11.115s, episode steps: 389, steps per second: 35, episode reward: 160.000, mean reward: 0.411 [0.000, 50.000], mean action: 6.812 [0.000, 13.000], mean observation: 2.693 [0.000, 240.000], loss: 24.809513, mean_absolute_error: 0.975669, mean_q: 0.999609\n",
      "  4804/20000: episode: 4, duration: 16.322s, episode steps: 574, steps per second: 35, episode reward: 730.000, mean reward: 1.272 [0.000, 100.000], mean action: 6.181 [0.000, 13.000], mean observation: 2.682 [0.000, 240.000], loss: 26.474405, mean_absolute_error: 0.980311, mean_q: 0.999667\n",
      "  6715/20000: episode: 5, duration: 54.243s, episode steps: 1911, steps per second: 35, episode reward: 1730.000, mean reward: 0.905 [0.000, 100.000], mean action: 6.465 [0.000, 13.000], mean observation: 1.966 [0.000, 240.000], loss: 27.552288, mean_absolute_error: 0.983224, mean_q: 0.999778\n",
      "  7436/20000: episode: 6, duration: 20.533s, episode steps: 721, steps per second: 35, episode reward: 630.000, mean reward: 0.874 [0.000, 100.000], mean action: 6.680 [0.000, 13.000], mean observation: 2.467 [0.000, 240.000], loss: 23.509558, mean_absolute_error: 0.976658, mean_q: 0.999871\n",
      "  9037/20000: episode: 7, duration: 46.187s, episode steps: 1601, steps per second: 35, episode reward: 1370.000, mean reward: 0.856 [0.000, 100.000], mean action: 6.362 [0.000, 13.000], mean observation: 1.978 [0.000, 240.000], loss: 28.201817, mean_absolute_error: 0.984314, mean_q: 0.999917\n",
      "  9814/20000: episode: 8, duration: 22.882s, episode steps: 777, steps per second: 34, episode reward: 510.000, mean reward: 0.656 [0.000, 100.000], mean action: 6.696 [0.000, 13.000], mean observation: 2.447 [0.000, 240.000], loss: 29.790602, mean_absolute_error: 0.986224, mean_q: 0.999949\n",
      " 10668/20000: episode: 9, duration: 24.384s, episode steps: 854, steps per second: 35, episode reward: 880.000, mean reward: 1.030 [0.000, 100.000], mean action: 6.515 [0.000, 13.000], mean observation: 2.054 [0.000, 240.000], loss: 25.584280, mean_absolute_error: 0.981066, mean_q: 0.999963\n",
      " 12412/20000: episode: 10, duration: 50.252s, episode steps: 1744, steps per second: 35, episode reward: 1470.000, mean reward: 0.843 [0.000, 100.000], mean action: 6.429 [0.000, 13.000], mean observation: 2.054 [0.000, 240.000], loss: 27.212114, mean_absolute_error: 0.984075, mean_q: 0.999976\n",
      " 14953/20000: episode: 11, duration: 72.592s, episode steps: 2541, steps per second: 35, episode reward: 1350.000, mean reward: 0.531 [0.000, 100.000], mean action: 6.522 [0.000, 13.000], mean observation: 1.552 [0.000, 240.000], loss: 27.300547, mean_absolute_error: 0.983247, mean_q: 0.999990\n",
      " 15628/20000: episode: 12, duration: 19.457s, episode steps: 675, steps per second: 35, episode reward: 480.000, mean reward: 0.711 [0.000, 100.000], mean action: 6.735 [0.000, 13.000], mean observation: 2.348 [0.000, 240.000], loss: 28.772602, mean_absolute_error: 0.985120, mean_q: 0.999995\n",
      " 16470/20000: episode: 13, duration: 24.316s, episode steps: 842, steps per second: 35, episode reward: 430.000, mean reward: 0.511 [0.000, 100.000], mean action: 6.429 [0.000, 13.000], mean observation: 3.008 [0.000, 240.000], loss: 27.028580, mean_absolute_error: 0.982211, mean_q: 0.999996\n",
      " 17149/20000: episode: 14, duration: 19.642s, episode steps: 679, steps per second: 35, episode reward: 730.000, mean reward: 1.075 [0.000, 100.000], mean action: 6.689 [0.000, 13.000], mean observation: 2.421 [0.000, 240.000], loss: 28.209648, mean_absolute_error: 0.984042, mean_q: 0.999997\n",
      " 18339/20000: episode: 15, duration: 34.086s, episode steps: 1190, steps per second: 35, episode reward: 780.000, mean reward: 0.655 [0.000, 100.000], mean action: 6.359 [0.000, 13.000], mean observation: 1.854 [0.000, 240.000], loss: 28.750597, mean_absolute_error: 0.984428, mean_q: 0.999998\n",
      "done, took 580.293 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x122d76c88>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Visualize the training during 20000 steps \n",
    "dqn.fit(env, nb_steps=20000, visualize=True, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the model \n",
    "dqn.save_weights('dqn_{}_weights.h5f'.format(ENV_NAME), overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 10 episodes ...\n",
      "Episode 1: reward: 140.000, steps: 2557\n",
      "Episode 2: reward: 140.000, steps: 2557\n",
      "Episode 3: reward: 140.000, steps: 2569\n",
      "Episode 4: reward: 140.000, steps: 2592\n",
      "Episode 5: reward: 140.000, steps: 2564\n",
      "Episode 6: reward: 140.000, steps: 2578\n",
      "Episode 7: reward: 140.000, steps: 2564\n",
      "Episode 8: reward: 140.000, steps: 2578\n",
      "Episode 9: reward: 140.000, steps: 2543\n",
      "Episode 10: reward: 140.000, steps: 2570\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12d79df98>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the algorithm for 10 episodes \n",
    "dqn.test(env, nb_episodes=10, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
