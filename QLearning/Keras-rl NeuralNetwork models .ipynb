{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras rl-neural network models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Model\n",
    "## Different models built on keras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 Model \n",
    "## DESCRIPTION : 6 layered Neural Network with dropout \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "\n",
    "def create_model_1():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, input_shape=(4,), activation='relu')) #Layer1 : 128 cells with relu activation function\n",
    "    model.add(Dropout(0.6))\n",
    "    \n",
    "    model.add(Dense(256, activation=\"relu\")) #Layer2 : 256 cells with relu activation function \n",
    "    model.add(Dropout(0.6))\n",
    "    \n",
    "    model.add(Dense(512, activation=\"relu\")) #Layer3 : 512 cells with relu activation function\n",
    "    model.add(Dropout(0.6))\n",
    "    \n",
    "    model.add(Dense(256, activation=\"relu\")) #Layer4 : 256 cells with relu activation function\n",
    "    model.add(Dropout(0.6))\n",
    "    \n",
    "    model.add(Dense(128, activation=\"relu\")) #Layer5 : 128 cells with relu activation function\n",
    "    model.add(Dropout(0.6))\n",
    "    model.add(Dense(2, activation=\"softmax\")) #Layer5 : softmax last layer transformation\n",
    "    \n",
    "    model.compile(                            #Layer6 : configure the learning process \n",
    "        loss=\"categorical_crossentropy\",\n",
    "        optimizer=\"adam\",\n",
    "        metrics=[\"accuracy\"])\n",
    "\n",
    "    print(model.summary())\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.2 Model \n",
    "## DESCRIPTION : dqn_atari \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten, Convolution2D, Permute \n",
    "\n",
    "def create_model_atari():\n",
    "    model_atari = Sequential()\n",
    "    model_atari.add(Convolution2D(32, 8, 8, subsample=(4, 4))) #Layer1 : convolutional layer 32 batch_size shape (8,8)\n",
    "    model_atari.add(Activation('relu'))\n",
    "    \n",
    "    model_atari.add(Convolution2D(64, 4, 4, subsample=(2, 2)))\n",
    "    model_atari.add(Activation('relu'))\n",
    "    \n",
    "    model_atari.add(Convolution2D(64, 3, 3, subsample=(1,1)))\n",
    "    model_atari.add(Activation('relu'))\n",
    "    \n",
    "    model_atari.add(Flatten())\n",
    "    model_atari.add(Dense(512))\n",
    "    \n",
    "    model_atari.add(Dense(nb_actions))\n",
    "    model_atari.add(Activation('linear'))\n",
    "\n",
    "    model.compile(                            #Layer6 : configure the learning process \n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=\"adam\",\n",
    "    metrics=[\"accuracy\"])\n",
    "    \n",
    "    return model\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Policies\n",
    "## Different policies implemented for keras "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LinearAnnealedPolicy  . kudos to matthiasplappert\n",
    "\n",
    "Wraps another policy and decreases a given parameter linearly. \n",
    "(This policy can be used together within EpsGreedyQPolicy to transform eps-value from 1 to 0.1 )\n",
    "\n",
    "#### EpsGreedyQPolicy\n",
    "\n",
    "Epsilon greedy policy is a way of selecting random actions with uniform distributions\n",
    "from a set of available actions. Using this policy either we can select random action\n",
    "with epsilon probability and we can select an action with 1-epsilon prob that gives maximum \n",
    "reward in a given state. \n",
    "As parameters we will select \n",
    "--epsilon  (eps-val) : probability of an event should occur  : from 0 to 1 ( makes an exploration-explotation that depends on this metric) \n",
    "\n",
    "#### GreedyQPolicy\n",
    "\n",
    "Epsilon greedy policy with epsilon value == 1\n",
    "\n",
    "#### BolztmanQPolicy\n",
    "\n",
    "Parameters \n",
    "--epsilon :\n",
    "\n",
    "#### MaxBoltzmanQPOlicy https://pure.uva.nl/ws/files/3153478/8461_UBA003000033.pdf\n",
    "\n",
    "A combination of the eps-greedy and Boltzman q-policy.\n",
    "\n",
    "\n",
    "#### BolztmannGumbelQPolicy https://arxiv.org/pdf/1705.10257.pdf\n",
    "\n",
    "BGE is invariant with respect to the mean of the rewards but not their\n",
    "   variance. The parameter C, which defaults to 1, can be used to correct for\n",
    "   this, and should be set to the least upper bound on the standard deviation\n",
    "   of the rewards.\n",
    "BGE is only available for training, not testing. For testing purposes, you\n",
    "    can achieve approximately the same result as BGE after training for N steps\n",
    "    on K actions with parameter C by using the BoltzmannQPolicy and setting\n",
    "    tau = C/sqrt(N/K)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
